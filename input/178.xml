<xMetaDiss:xMetaDiss xmlns:xMetaDiss="http://www.d-nb.de/standards/xmetadissplus/" xmlns:cc="http://www.d-nb.de/standards/cc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcmitype="http://purl.org/dc/dcmitype/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:pc="http://www.d-nb.de/standards/pc/" xmlns:urn="http://www.d-nb.de/standards/urn/" xmlns:hdl="http://www.d-nb.de/standards/hdl/" xmlns:doi="http://www.d-nb.de/standards/doi/" xmlns:thesis="http://www.ndltd.org/standards/metadata/etdms/1.0/" xmlns:ddb="http://www.d-nb.de/standards/ddb/" xmlns:dini="http://www.d-nb.de/standards/xmetadissplus/type/" xmlns="http://www.d-nb.de/standards/subject/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:doc="http://www.lyncode.com/xoai" xsi:schemaLocation="http://www.d-nb.de/standards/xmetadissplus/  http://files.dnb.de/standards/xmetadissplus/xmetadissplus.xsd"><id>178</id>
   <dc:title xsi:type="ddb:titleISO639-2" lang="eng">3D time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-technology</dc:title>
   <dc:creator xsi:type="pc:MetaPers">
      <pc:person>
         <pc:name type="nameUsedByThePerson">
            <pc:foreName>Robert</pc:foreName>
            <pc:surName>Lange</pc:surName>
         </pc:name>
      </pc:person>
   </dc:creator>
   <dc:subject xsi:type="xMetaDiss:noScheme">3D-Kamera</dc:subject>
   <dc:subject xsi:type="xMetaDiss:noScheme">(Echo-) Laufzeitverfahren</dc:subject>
   <dc:subject xsi:type="xMetaDiss:noScheme">Distanzmessung</dc:subject>
   <dc:subject xsi:type="xMetaDiss:DDC-SG">620</dc:subject>
   <dcterms:abstract xsi:type="ddb:contentISO639-2" lang="ger">Three-D time-of-flight distance measurement with custom solid-state image sensors in CMOS/CCD-technology &#13;
Da wir in einer dreidimensionalen Welt leben, erfordert eine geeignete Beschreibung unserer Umwelt für viele Anwendungen Kenntnis über die relative Position und Bewegung der verschiedenen Objekte innerhalb einer Szene. Die daraus resultierende Anforderung räumlicher Wahrnehmung ist in der Natur dadurch gelöst, daß die meisten Tiere mindestens zwei Augen haben. Diese Fähigkeit des Stereosehens bildet die Basis dafür, daß unser Gehirn qualitative Tiefeninformationen berechnen kann. Ein anderer wichtiger Parameter innerhalb des komplexen menschlichen 3D-Sehens ist unser Erinnerungsvermögen und unsere Erfahrung. Der Mensch ist sogar in der Lage auch ohne Stereosehen Tiefeninformation zu erkennen. Beispielsweise können wir von den meisten Photos, vorausgesetzt sie bilden Objekte ab, die wir bereits kennen, die 3D Information im Kopf rekonstruieren [COR].  &#13;
Die Aufnahme, Speicherung, Weiterverarbeitung und der Vergleich dieser riesigen Datenmengen erfordert eine enorme Rechenleistung. Glücklicherweise stellt uns die Natur diese Rechenleistung zur Verfügung. Für eine technische Realisierung sollten wir aber nach einfacheren und genaueren Meßprinzipien suchen.  &#13;
Bildgebende 3D Meßmethoden mit einer brauchbaren Distanzauflösung sind bisher nur in Form von passiven (z.B. Stereosehen) oder aktiven (z.B. Streifenprojektionsverfahren) Triangulationssystemen realisiert worden. Solche Triangulationssysteme bringen vor allem die Nachteile der Abschattungsproblematik und der Mehrdeutigkeit (Streifenprojektion) mit sich. Somit sind oftmals die möglichen Einsatzgebiete eingeschränkt. Außerdem erfordert Stereosehen kontrastreiche Szenen, denn sein Grundprinzip besteht in&#13;
der Extrahierung bestimmter signifikanter (kontrastreicher) Merkmale innerhalb der Szenen und dem Positionsvergleich dieser Merkmale in den beiden Bildern. Überdies erfordert die Gewinnung der 3D Information einen hohen Rechenaufwand. Hohe Meßauflösung hingegen kann man nur mit einer großen Triangulationsbasis gewährleisten, welche wiederum zu großen Kamerasystemen führt.  &#13;
Eine elegantere Methode zur Entfernungsmessung ist das „Time-of-Flight (TOF)“- Verfahren (Fluglaufzeitverfahren), ein optisches Analogon zum Ultraschall Navigationssystem der Fledermaus. Bisher wird das TOF- Verfahren nur eindimensional eingesetzt, also für die Distanzbestimmung zwischen zwei Punkten. Um mit solchen 1D Meßsystemen die 3D Information der Szene zu erlangen,&#13;
benutzt man Laserscanner. Diese sind aber teuer, groß, verhältnismäßig langsam und empfindlich gegen Erschütterungen und Vibrationen. Scannende TOF- Systeme sind daher nur für eine eingeschränkte Anzahl von Applikationen geeignet.&#13;
In dieser Dissertation stellen wir erstmals eine nicht scannende bildgebende 3D-Kamera vor, die nach dem TOF- Prinzip arbeitet und auf einem Array von sogenannten Demodulationspixeln beruht. Jedes dieser Pixel ermöglicht sowohl die Messung der Hintergrundintensität als auch die individuelle Ankunftszeit einer HF-modulierten Szenenbeleuchtung mit einer Genauigkeit von wenigen hundert Pikosekunden. Das Funktionsprinzip der Pixel basiert auf dem CCD Prinzip ( C harge C oupled D evice), welches den Transport, die Speicherung und die Akkumulation optisch generierter Ladungsträger in definierten örtlich begrenzten Gebieten auf dem Bildsensor erlaubt. Ladungstransport und -addition können von CCDs enorm schnell und beinahe verlustfrei durchgeführt werden. Wir bezeichnen diese neuartigen, hochfunktionalen und leistungsstarken Pixel als Demodulationspixel , weil man mit jedem von ihnen die Entfernungs- und Reflektivitätsinformation des zu vermessenden Ziels aus dem empfangenen optischen Signal extrahieren kann. Die gesuchte Information wird dem aktiven optischen Signal während der Ausbreitung des Lichts durch die Szene (Time of Flight) aufmoduliert. Jedes Pixe l arbeitet wie eine individuelle Hochpräzisions- Stoppuhr. Da die Realisierung im wesentlichen auf CMOS- Technologie basiert, wird diese neue Technik von den stetig fortschreitenden Technologieentwicklungen und -verbesserungen profitieren und zwar in Form von besserer Zeitauflösung und damit höherer Distanzgenauigkeit. Dank der Benutzung einer CMOS- Technologie können zukünftig sehr leicht auch alle bekannten CMOS-APS- Eigenschaften ( A ctive P ixel S ensor) monolithisch implementiert werden (z.B. Definition und Auslese von Bildsegmenten: regions of interest , A/D Wandlung auf dem Sensor, …). &#13;
Die neuen Bildsensoren sind in einem 2 µm CMOS/CCD Prozess hergestellt worden, einem leicht modifizierten CMOS Prozess, der zur kostengünstigen Prototypenfertigung zur Verfügung steht (sogenannte MPWs, M ulti P roject W afer). Dieser Prozess bietet die Möglichkeit, CCD Strukturen zu realisieren. Obwohl diese CCDs nicht die Qualität spezieller CCD- Prozesse erreichen, genügen sie den Anforderungen unserer Anwendung vollkommen. Wir haben verschiedene Pixelstrukturen realisiert und charakterisiert und&#13;
präsentieren in dieser Arbeit die Ergebnisse. Das Demodulationspixel mit dem besten Füllfaktor und den effizientesten Demodulationseigenschaften wurde als Zeilensensor mit 108 Pixeln und als Bildsensor mit 64 x 25 Pixeln fabriziert. Beide Sensoren sind in separaten Entfernungskameras implementiert, die jeweils modulierte LEDs als Lichtquelle benutzen und einen Entfernungsbereich von 7.5 Metern oder sogar 15 Metern abdecken. Für nicht kooperative diffus reflektierende Ziele erreichen beide Kameras eine Auflösung von wenigen Zentimetern. Mit Ausnahme der Bildsensoren werden in den Distanzkameras ausschließlich optische und elektrische Standardkomponenten eingesetzt. Bei einer Integrationszeit von 50 ms (20 Hz 3D-Bild-Wiederholrate) genügt für eine Distanzauflösung von 5 Zentimetern eine optische Leistung von 600 Femtowatt pro Pixel (Wellenlänge des Lichts: 630 nm). Bei dieser niedrigen optischen Leistung werden statistisch lediglich 0.06 Elektronen innerhalb einer Modulationsperiode von 50 ns akkumuliert (20 MHz Modulationsfrequenz), also nur ein Elektron in jeder 16ten Periode.  &#13;
Wir führen eine ausführliche Analyse der Einflüsse von Nichtlinearitäten innerhalb der Elektronik, von Aliasing Effekten, von der Integrationszeit und von den Modulationssignalen durch. Außerdem stellen wir eine optische Leistungsbilanz auf und präsentieren eine Formel zur Voraussage der Distanzauflösung als Funktion des Verhältnisses der Hintergrundhelligkeit zur Intensität des aktiven optischen Signals sowie weiteren Kamera- und Szenenparametern. Die Gültigkeit dieser Formel wird durch Simulationen und echte&#13;
Messungen verifiziert, so daß wir in der Lage sind, für eine vorgegebene Integrationszeit, optische Leistung und Zieldistanz und -reflektivität die Meßgenauigkeit des Systems vorherzusagen.  &#13;
Wir demonstrieren die ersten erfolgreichen Realisierungen bildgebender, rein elektronischer 3D-Entfernungskameras nach dem Laufzeitprinzip ohne bewegte Teile, welche auf kundenspezifischen PhotoASICS beruhen. Die erzielten Meßresultate dieser Kameras sind beinahe ausschließlich vom natürlichen Quantenrauschen das Lichts limitiert. Wir zeigen, daß das optische 3D TOF- Verfahren ein exzellentes kostengünstiges Werkzeug für alle modernen berührungslosen optischen Meßaufgaben zur Überwachung relativer&#13;
Objektpositionen oder -bewegungen ist.</dcterms:abstract>
   <dcterms:abstract xsi:type="ddb:contentISO639-2" lang="eng">Since we are living in a three-dimensional world, an adequate description of our environment for many applications includes the relative position and motion of the different objects in a scene. Nature has satisfied this need for spatial perception by providing most animals with at least two eyes. This stereo vision ability is the basis that allows the brain to calculate qualitative depth information of the observed scene. Another important parameter in the complex human depth perception is our experience and memory. Although it is far more difficult, a human being is even able to recognize depth information without stereo vision. For example, we can qualitatively deduce the 3D scene from most photos, assuming that the photos contain known objects [COR]. The acquisition, storage, processing and comparison of such a huge amount of information requires enormous computational power - with which nature fortunately provides us. Therefore, for a technical implementation, one should resort to other simpler measurement principles. Additionally, the qualitative distance estimates of such knowledge-based passive vision systems can be replaced by accurate range measurements. Imaging 3D measurement with useful distance resolution has mainly been realized so far with triangulation systems, either passive triangulation (stereo vision) or active triangulation (e.g. projected fringe methods). These triangulation systems have to deal with shadowing effects and ambiguity problems (projected fringe), which often restrict the range of application areas. Moreover, stereo vision cannot be used to measure a contrastless scene. This is because the basic principle of stereo vision is the extraction of characteristic contrast-related features within the observed scene and the comparison of their position within the two images. Also, extracting the 3D information from the measured data requires an enormous time-consuming computational effort. High resolution can only be achieved with a relatively large triangulation base and hence large camera systems. A smarter range measurement method is the TOF ( T ime- O f- F light) principle, an optical analogy to a bat´s ultrasonic system rather than human´s stereo vision. So far TOF systems are only available as 1D systems (point measurement), requiring laser scanners to acquire 3D images. Such TOF scanners are expensive, bulky, slow, vibration sensitive and therefore only suited for restricted application fields. In this dissertation an imaging, i.e. non-scanning TOF-camera is introduced, based on an array of demodulation pixels, where each pixel can measure both the background intensity and the individual arrival time of an RF-modulated (20 MHz) scene illumination with an accuracy of a few hundreds of picoseconds (300⋅10 -12 s). The pixel´s working concept is based on the CCD principle ( C harge C oupled D evice), allowing the transportation, storage and accumulation of optically generated charge carriers to defined local sites within the imaging device. This process is extremely fast and essentially loss-free. We call our new, powerful high-functionality pixels demodulation pixels because they extract the target´s distance and reflectivity from the received optical signal. This extracted information is modulated into the active optical signal during the time of propagation of the light (or time of flight) through the observed scene. Each pixel works like an individual high-precision stopwatch, and since its realization is mainly based on CMOS technology this new technique will benefit from the ongoing technology developments in terms of improved time- and hence distance resolution. Thanks to the use of CMOS, all commonly known CMOS APS ( A ctive P ixel S ensor) features ( R egions O f I nterest addressing: ROI, AD conversion, etc.) can be implemented monolithically in the future. The imaging devices have been fabricated in a 2 µm CMOS/CCD process, a slightly modified CMOS process which is available as an inexpensive prototyping service ( M ulti P roject W afer: MPW). This process offers the freedom to implement CCDs with sufficiently good performance for our application, although the performance is inferior to dedicated CCD technologies. We have realized and characterized several different pixel structures and will present these results here. The demodulation pixel with the best fill-factor and demodulation performance has been implemented (1) as a line sensor with 108 pixels and (2) as an image sensor with 64 x 25 pixels. Both devices have been integrated in separate range cameras working with modulated LED illumination and covering a distance range of 7.5 up to 15 meters. For non-cooperative diffusely reflecting targets these cameras achieve centimeter accuracy. With the single exception of the demodulation pixel array itself, only standard electronic and optical components have been used in these range cameras. For a resolution of 5 centimeters, an optical power of 600 fW per pixel is sufficient, assuming an integration time of 50 ms (20 Hz frame rate of 3D images). This low optical power implies that only 0.06 electrons are generated per modulation period (T mod =50 ns at 20 MHz modulation frequency). Furthermore, we present an in-depth analysis of the influences of non-linearities in the electronics, aliasing effects, integration time and modulation functions. Also, an optical power budget and a prediction for the range accuracy is derived as a function of the ratio of active illumination to background illumination. The validity of this equation is confirmed by both computer simulations and experimental measurements with real devices. Thus, we are able to predict the range accuracy for given integration time, optical power, target distance and reflectance. With this work we demonstrate the first successful realization of an all-solid-state 3D TOF range-camera without moving parts that is based on a dedicated customized PhotoASIC. The measured performance is very close to the theoretical limits. We clearly demonstrate that optical 3D-TOF is an excellent, cost-effective tool for all modern vision problems, where the relative position or motion of objects need to be monitored.</dcterms:abstract>
   <dc:publisher xsi:type="cc:Publisher" type="dcterms:ISO3166" countryCode="DE">
      <cc:universityOrInstitution cc:GKD-Nr="509551-7">
         <cc:name>Universitätsbibliothek der Universität Siegen</cc:name>
         <cc:place>Siegen</cc:place>
      </cc:universityOrInstitution>
      <cc:address cc:Scheme="DIN5008">Adolf-Reichweinstr. 2, 57068 Siegen</cc:address>
   </dc:publisher>
   <dcterms:dateAccepted xsi:type="dcterms:W3CDTF">2000-09-07</dcterms:dateAccepted>
   <dcterms:issued xsi:type="dcterms:W3CDTF">2000</dcterms:issued>
   <dc:type xsi:type="dini:PublType">doctoralThesis</dc:type>
   <dc:type xsi:type="dcterms:DCMIType">Text</dc:type>
   <dini:version_driver>publishedVersion</dini:version_driver>
   <dc:identifier xsi:type="urn:nbn">urn:nbn:de:hbz:467-1783</dc:identifier>
   <dc:language xsi:type="dcterms:ISO639-2">eng</dc:language>
   <dc:rights xsi:type="dcterms:URI">https://dspace.ub.uni-siegen.de/static/license.txt</dc:rights>
   <thesis:degree>
      <thesis:level>thesis.doctoral</thesis:level>
      <thesis:grantor xsi:type="cc:Corporate" type="dcterms:ISO3166" countryCode="DE">
         <cc:universityOrInstitution>
            <cc:name>Universität Siegen</cc:name>
            <cc:place>Siegen</cc:place>
            <cc:department>
               <cc:name>Fachbereich 12, Elektrotechnik und Informatik</cc:name>
               <cc:place>Siegen</cc:place>
            </cc:department>
         </cc:universityOrInstitution>
      </thesis:grantor>
   </thesis:degree>
   <ddb:contact ddb:contactID="L6000-0732"/>
   <ddb:fileNumber>2</ddb:fileNumber>
   <ddb:fileProperties ddb:fileName="lange.pdf" ddb:fileSize="5069896"/>
   <ddb:fileProperties ddb:fileName="3D_movie.exe" ddb:fileSize="3620038"/>
   <ddb:identifier ddb:type="URL">https://dspace.ub.uni-siegen.de/handle/ubsi/178</ddb:identifier>
   <ddb:rights ddb:kind="free"/>
   <ddb:server>Universitätsbibliothek Siegen</ddb:server>
</xMetaDiss:xMetaDiss>